# Stowage

[![Nuget](https://img.shields.io/nuget/v/Stowage?style=for-the-badge&label=stable)](https://www.nuget.org/packages/Stowage) [![Nuget (with prereleases)](https://img.shields.io/nuget/vpre/Stowage?style=for-the-badge&label=pre-release)](https://www.nuget.org/packages/Stowage) [![Nuget](https://img.shields.io/nuget/dt/Stowage?style=for-the-badge)](https://www.nuget.org/stats/packages/Stowage?groupby=Version)

![logo](../media/icon/icon-256.png)

> This documentation is for Stowage v2 which is a major redesign. Version 1 documentation can be found [here](README.v1.md).

**Stowage** is a **bloat-free .NET cloud storage kit** that supports at minimum THE major ☁ providers.

- **Independent** 🆓. Provides an independent implementation of the ☁ storage APIs. Because you can't just have official corporate SDKs as a single source of truth.
- **Readable**. Official SDKs like the ones for [AWS](https://github.com/aws/aws-sdk-net), [Google](https://github.com/googleapis/google-cloud-dotnet), or [Azure](https://github.com/Azure/azure-storage-net) are overengineered and unreadable. Some are autogenerated and look just bad and foreign to .NET ecosystem. Some won't even compile without some custom rituals.
- **Beautiful** 🦋. Designed to fit into .NET ecosystem, not the other way around.
- **Rich** 💰. Provide maximum functionality. However, in addition to that, provide humanly possible way to easily extend it with new functionality, without waiting for new SDK releases.
- **Embeddable** 🔱. Has zero external dependencies, relies only on built-in .NET API. Often official SDKs have a very deep dependency tree causing a large binary sizes and endless conflicts during runtime. This one is a single .NET .dll with no dependencies whatsoever.
- **Cross Cloud** 🌥. Same API. Any cloud. Best decisions made for you. It's like iPhone vs Windows Phone.
- **Cross Tested** ❎. It's not just cross cloud but also cross tested (I don't know how to call this). It tests that all cloud providers behave absolutely the same on various method calls. They should validate arguments the same, throw same exceptions in the same situations, and support the same set of functionality. Sounds simple, but it's rare to find in a library. And it important, otherwise what's the point of a generic API if you need to write a lot of `if()`s? (or pattern matching).

> This library originally came out from being frustrated on working on my another library - [Storage.Net](https://github.com/aloneguid/storage). While it's OK, most of the time I had to deal with SDK incompatibilities, breaking changes, oddnesses, and slowness, whereas most of the time users needs something simple that just works.

## Getting Started

Right, time to gear up. We'll do it [step by step](https://www.oxfordlearnersdictionaries.com/definition/english/step_1?q=step). First, you need to [install](https://docs.microsoft.com/en-us/nuget/quickstart/install-and-use-a-package-using-the-dotnet-cli) the [![Nuget](https://img.shields.io/nuget/v/Stowage?style=social)](https://www.nuget.org/packages/Stowage) package.


Simplest case, using the local 💽 and writing text "I'm a page!!!" to a file called "pagefile.sys" at the root of disk C::

```csharp
using Stowage;

using(IFileStorage fs = Files.Of.LocalDisk("c:\\")) {
   await fs.WriteText("pagefile.sys", "I'm a page!!!!");
}
```

This is local disk, yeah? But what about cloud storage, like Azure Blob Storage? Piece of cake:

```csharp
using Stowage;

using(IFileStorage fs = Files.Of.AzureBlobStorage("accountName", "accountKey", "containerName")) {
   var entries = await fs.Ls();
}
```



## ♒ <span style="color:red">S</span>treaming

Streaming is a *first-class feature*. This means the streaming is real with no workarounds or in-memory buffers, so you can upload/download files of virtually unlimited sizes. Most official SDKs *do not support streaming* at all - surprisingly even the cloud leader's .NET SDK doesn't. Each requires some sort of crippled down version of stream - either knowing length beforehand, or will buffer it all in memory. I don't. I stream like a stream.

Proper streaming support also means that you can transform streams as you write to them or read from them - something that is not available in the native SDKs. For instance gzipping, encryption, anything else.

Streaming is also truly compatible with synchronous and asynchronous API.

## Details/Documentation

Whenever a method appears here, I assume it belongs to `IFileStorage` interface, unless specified.

### Listing/Browsing

Use `.Ls()` ([short for list](https://en.wikipedia.org/wiki/Ls)) - very easy to remember! Everyone knows what **ls** does, right? Optionally allows to list entries recursively.

### Reading

The core method for reading is `Stream OpenRead(IOPath path)` - this returns a stream from file path. Stream is the lowest level data structure. There are other helper methods that by default rely on this method, like `ReadText` etc. Just have a quick look:

```csharp
IFileStorage fs = ...;
Stream target = ...;

// copy to another stream
using Stream s = await fs.OpenRead("/myfile.txt");

// synchronous copy:
s.CopyTo(target);

// or alternatively, asynchronous copy (preferred):
await s.CopyToAsync(target);

// if you just need text:
string content = await fs.ReadText("/myfile.txt");
```

Of course there are more overloaded methods you can take advantage of.

### Writing

The main method `Stream OpenWrite(IOPath path, ...)` opens(/creates?) a file for writing. It returns a real writeable stream you can write to and close afterwards. It behaves like a stream and is a stream.

There are other overloads which support writing text etc.

### Destroying 🧨

`Rm(IOPath path)` trashes files or folders (or both) with options to do it recursively!

### Other

There are other useful utility methods:

- `bool Exists(IOPath path)` that checks for file existence. It supposed to be really efficient, hence a separate method.
- `Ren` renames files and folders.
- and more are coming - check `IFileStorage` interface to be up to date.

## Supported Storage Systems (Built-In)

- Local Disk Directory (`Files.Of.LocalDisk(...)`).
- In-Memory (`Files.Of.InternalMemory(...)`).
- [AWS S3](https://aws.amazon.com/s3/) (`Files.Of.AmazonS3(...)`).
  - [Minio](https://min.io/) (`Files.Of.Minio(...)`).
  - [DigitalOcean Spaces](https://www.digitalocean.com/products/spaces) (`Files.Of.DigitalOceanSpaces(...)`).

- [Azure Blob Storage](https://azure.microsoft.com/en-gb/services/storage/blobs/) / [Data Lake Gen 2](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#key-features-of-data-lake-storage-gen2) (`Files.Of.AzureBlobStorage(...)`).
- [Google Cloud Storage](https://cloud.google.com/storage) (`Files.Of.GoogleCloudStorage(...)`).
- [Databricks DBFS](https://docs.databricks.com/data/databricks-file-system.html) (`Files.Of.DatabricksDbfs(...)`).

Instantiation instructions are in the code documentation ([IntelliSense](https://docs.microsoft.com/en-us/visualstudio/ide/using-intellisense?view=vs-2019)?) - I prefer this to writing out here locally.

Below are some details worth mentioning.

### AWS S3

In AWS, the path addressing style is the following:

```
/bucket/path/object
```

`Ls` on the root folder returns *list of buckets* in the AWS account, whether you do have access to them or not.

#### Authentication

 The most usual way to authenticate with S3 is to use the following method:

```csharp
IFileStorage storage = Files.Of.AmazonS3(key, secret, region);
```

These are what Amazon calls "long-term" credentials. If you are using STS, the same method overload allows you to pass `sessionToken`.

Another way to authenticate is using CLI profile. This is useful when you machine is already authenticated using [aws cli](https://aws.amazon.com/cli/), [awsume](https://awsu.me/) or similar tools that write credentials and configuration to `~/.aws/credentials` and `~/.aws/config`. 

You only need to pass the profile name (and only if it's not a default one):

```csharp
IFileStorage storage = Files.Of.AmazonS3FromCliProfile();
```

This method has other default parameters, such as `regionName` which can be specified or overridden if not found in CLI configuration.

##### Minio

Minio is essentially using the standard S3 protocol, but addressing style is slightly different. There is a helper extension that somewhat simplifies Minio authentication:

```csharp
IFileStorage storage = Files.Of.Minio(endpoint, key, secret);
```

### Azure Blob Storage

In Azure Blob Storage, path addressing style is the following:

```
/container/path/object
```

Note that there is no storage account in the path, mostly because *Shared Key authentication is storage account scoped*, not tenant scoped.

`Ls` on the root folder returns *list of containers* in the storage account.

#### Authentication

Azure provider supports authentication with [Shared Key](https://docs.microsoft.com/en-us/rest/api/storageservices/authorize-with-shared-key):

```csharp
IFileStorage storage = Files.Of.AzureBlobStorage(accountName, sharedKey);
```

Since v2, authentication with Entra Id service principals is supported too:

```csharp
IFileStorage storage = Files.Of.AzureBlobStorage(
    accountName,
    new ClientSecretCredential(tenantId, clientId, clientSecret));
```

Interactive authentication with user credentials, and managed identities are not yet supported, but watch this space.

#### Emulator

Azure emulator is supported, just use `AzureBlobStorageWithLocalEmulator()` method to connect to it.

### Exotic providers

#### Local disk cache

This storage essentially wraps around another storage to provide content caching capabilities. Example:

```csharp
IFileStorage storage = Files.Of.AzureBlobStorage(accountName, sharedKey);
IFileStorage cachedStorage = Files.Of.LocalDiskCacheStorage(storage);
```

When using `cachedStorage`, all the operations are forwarded to `storage` as is, except for `OpenRead` which downloads content locally and opens a stream to the local file.

## 🦓 Connection Strings

You can also use connection strings, which are useful when implementation type is unknown beforehand, should be configurable, or you just don't want to implement implementation factory yourself. To create a storage using connection string use the following method:

```csharp
IFileStorage storage = Files.Of.ConnectionString(connectionString);
```

Connection strings have the following format: `<prefix>://<parameters>`.

Prefix is implementation type, like `disk`, `s3` and so on, and parameters are implementation specific.

```mermaid
mindmap
	root((CS))
		AWS S3
			prefix
				s3
			connection types
				AWS CLI profiles
					examples
							default profile
								s31["`s3://`"]
								s32["`s3://profile=default`"]
							specific profile
								s33["`s3://profile=name`"]
                    optional parameters
                        region
                            if not specified, must be in cli profile
				using keys
					examples
						s34["`s3://keyId=...;key=...;region=...`"]
		local disk
			prefix
				disk
			connection types
				entire disk
					disk2["`disk://`"]
				specific directory
					disk1["`disk://path=localPath`"]
		in-memory
		azure blobs
		DBFS
						
```





## 📈 Extending

There are many ways to extend functionality:

1. **Documentation**. You might think it's not extending anything, however if user is not aware for some functionality it doesn't exist. Documenting it is making it available, hence extending. You must be slightly mad to follow my style of writing though.
2. **New functionality**. Adding utility methods like copying files inside or between accounts, automatic JSON serialisation etc. is always good. Look [`IFileStorage`](src/Stowage/IFileStorage.cs) interface and [`PolyfilledFileStorage`](src/Stowage/PolyfilledFileStorage.cs). In most cases these two files are enough to add pure business logic. Not counting unit tests. Which you must write. Otherwise it's easier to do the whole thing by myself. Which is what will happen according to my experience.
3. **Native optimisations**. Some functionality is generic, and some depends on a specific cloud provider. For instance, one can copy a file by downloading it locally, and uploading with a new name. Or utilise a native REST call that accepts source and target file name, if it exists. Involves digging deeper into specific provider's API.

When contributing a new provider, it's way more preferrable to embed it's code in the library, provided that:

- there are no extra nuget dependencies.
- it's cross-platform.

I'm a strong advocate of simplicity and not going to repeat the mistake of turning this into a nuget tree dependency hell!

## ❔ Who?

- Featured in [The .NET MAUI Podcast, episode 98](https://www.dotnetmauipodcast.com/98).
- Blog post [Exploring Stowage](https://developingdane.com/exploring-stowage/).

Raise a PR to appear here.

## Related Projects

- [R**CLONE**](https://rclone.org/) - cross-platform open-source cloud sync tool.
- [Storage.Net](https://www.aloneguid.uk/projects/storagenet/) - the roots of this project.

## 💰 Contributing

You are welcome to contribute in any form, however I wouldn't bother, especially financially. Don't bother buying me a ☕, I can do it myself real cheap. During my years of OSS development everyone I know (including myself) have only lost money. Why I'm still doing this? Probably because *it's just cool and I'm enjoying it*.
